## Домашняя работа 4
Spark приложение на Scala
### Задание 1:
Загрузить данные в первый DataFrame из файла с фактическими данными поездок в Parquet (src/main/resources/data/yellow_taxi_jan_25_2018). Загрузить данные во второй DataFrame из файла со справочными данными поездок в csv (src/main/resources/data/taxi_zones.csv) С помощью DSL построить таблицу, которая покажет какие районы самые популярные для заказов. Результат вывести на экран и записать в файл Паркет.
#### Результат: 
В консоли должны появиться данные с результирующей таблицей, в файловой системе должен появиться файл.

### Задание 2:
Загрузить данные в RDD из файла с фактическими данными поездок в Parquet (src/main/resources/data/yellow_taxi_jan_25_2018). С помощью lambda построить таблицу, которая покажет В какое время происходит больше всего вызовов. Результат вывести на экран и в txt файл c пробелами.

#### Результат: 
В консоли должны появиться данные с результирующей таблицей, в файловой системе должен появиться файл.

### Задание 3:
Загрузить данные в DataSet из файла с фактическими данными поездок в Parquet (src/main/resources/data/yellow_taxi_jan_25_2018). С помощью DSL и lambda построить таблицу, которая покажет. Как происходит распределение поездок по дистанции? Результат вывести на экран и записать в бд Постгрес (докер в проекте). Для записи в базу данных необходимо продумать и также приложить инит sql файл со структурой.

Построить витрину со следующими колонками: 
общее количество поездок,
среднее расстояние,
среднеквадратическое отклонение, 
минимальное и максимальное расстояние

#### Результат: 
В консоли должны появиться данные с результирующей таблицей, в бд должна появиться таблица.


### Запуск приложения из Idea:
Предварительно
```
docker-compose up -d
```
Нужно установить флаг “Include dependencies with "Provided" scope”
```
Run hw4_spark_api/src/main/scala/Main.scala
```
